{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'reconstruction_net' object has no attribute 'conv3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\OneDrive - Danmarks Tekniske Universitet\\Dokumenter\\GitHub\\Collab-KTC2023\\Codes_Python\\neural_net2.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m net \u001b[39m=\u001b[39m reconstruction_net()\n",
      "\u001b[1;32mc:\\Users\\marti\\OneDrive - Danmarks Tekniske Universitet\\Dokumenter\\GitHub\\Collab-KTC2023\\Codes_Python\\neural_net2.ipynb Cell 2\u001b[0m line \u001b[0;36mreconstruction_net.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m16\u001b[39m \u001b[39m*\u001b[39m \u001b[39m16\u001b[39m \u001b[39m*\u001b[39m \u001b[39m16\u001b[39m, \u001b[39m120\u001b[39m)  \u001b[39m# 5*5 from image dimension\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinClas \u001b[39m=\u001b[39m data_net()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(\u001b[39m131\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(\u001b[39m6\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m16\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Dokumenter/GitHub/Collab-KTC2023/Codes_Python/neural_net2.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m256\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'reconstruction_net' object has no attribute 'conv3'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class data_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        N_data = 2300\n",
    "        super(data_net, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(N_data, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 16) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "class reconstruction_net(nn.Module):\n",
    "    # Note that downsizing/upsizing doesn't match\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 16x16 square convolution\n",
    "        # kernel\n",
    "        self.softmax = nn.Softmax\n",
    "        self.conv1 = nn.Conv2d(1, 6, 256) # 1 input channel, 6 output channels, 256 by 256 image.\n",
    "        self.conv2 = nn.Conv2d(6, 16, 16) # Need to do some pooling to get down to 16 by 16 image.\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 120)  # 16*16 from image dimension\n",
    "        self.data_branch = data_net()\n",
    "        self.conv3(136,6,1)\n",
    "        self.conv4(6,3,16)\n",
    "        self.conv5(3,3,256) \n",
    "        #self.fc2 = nn.Linear(121, 84)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        # Replace F with self\n",
    "        x, data = inputs    # split tuple\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        # Concatenate with BinaryClassification\n",
    "        x = torch.cat([F.relu(self.fc1(x)), self.data_branch(data)])\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.softmax(self.conv5(x))\n",
    "        return x\n",
    "net = reconstruction_net()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
